{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6a76c9-69f8-4a49-b3b1-06a5101aef04",
   "metadata": {},
   "source": [
    "# Atelier 1: Introduction aux LLMs et aux SLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b3e5ac-d5a7-45a3-808c-cd85e86ef300",
   "metadata": {},
   "source": [
    "## Table des matières:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedfaa3-d377-407c-8cd3-385d4dce05df",
   "metadata": {},
   "source": [
    "1. Introduction aux LLMs et SLMs\n",
    "2. Utilisation basique d'un LLM\n",
    "3. Utilisation d'un SLM\n",
    "4. Conclusion et ressources supplémentaires\n",
    "\n",
    "---\n",
    "\n",
    "Bienvenue à cet atelier sur les modèles de langage! Aujourd'hui, nous allons explorer les différences entre les grands modèles de langage (LLMs) et les petits modèles de langage (SLMs), et apprendre à les utiliser dans des applications pratiques.\n",
    "\n",
    "**Objectifs de l'atelier:**\n",
    "- Comprendre les différences entre LLMs et SLMs\n",
    "- Apprendre à interagir avec un SLM local\n",
    "- Créer des prompts efficaces\n",
    "- Explorer les paramètres qui influencent les réponses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a179cd-b795-4ed8-9ee1-73b09225bba8",
   "metadata": {},
   "source": [
    "## 1. Introduction aux LLMs et SLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f46263-fa9d-487b-9f45-f57eced81d26",
   "metadata": {},
   "source": [
    "### Qu'est-ce qu'un LLM (Large Language Model)?\n",
    "- Modèle de langage de grande taille (des dizaines ou centaines de milliards de paramètres)\n",
    "- Exemples: GPT-4, Claude, PaLM, Llama 2 (70B)\n",
    "- Nécessite généralement une infrastructure cloud pour fonctionner\n",
    "- Performances élevées sur une large gamme de tâches\n",
    "\n",
    "### Qu'est-ce qu'un SLM (Small Language Model)?\n",
    "- Modèle de langage de taille réduite (moins de 13 milliards de paramètres)\n",
    "- Exemples: Mistral 7B (Français), LiquidAI, Phi-2, Llama 2 (7B)\n",
    "- Peut fonctionner sur un ordinateur personnel\n",
    "- Performances plus limitées mais suffisantes pour de nombreuses applications\n",
    "\n",
    "### Avantages des SLMs:\n",
    "- Confidentialité des données (exécution locale)\n",
    "- Pas de coûts d'API\n",
    "- Personnalisation possible\n",
    "- Utilisation hors ligne\n",
    "- Contrôle total sur les paramètres\n",
    "\n",
    "### Limitations des SLMs:\n",
    "- Connaissances plus limitées\n",
    "- Capacités de raisonnement réduites\n",
    "- Hallucinations plus fréquentes\n",
    "- Contexte plus court"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0424e3-7b07-44e4-96b4-64444f6c2b4a",
   "metadata": {},
   "source": [
    "## 2. Configuration de l'environnement\n",
    "\n",
    "Dans cet atelier, nous utilisons:\n",
    "- Un serveur local llama.cpp qui expose une API compatible avec OpenAI\n",
    "- La bibliothèque Python `openai` pour communiquer avec ce serveur\n",
    "\n",
    "Le serveur llama.cpp devrait déjà être en cours d'exécution sur le port 8080 avec le modèle que nous avons téléchargé.\n",
    "\n",
    "Commençons par installer et configurer la bibliothèque OpenAI pour communiquer avec notre serveur local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c16077-cd59-4128-849f-c7b93f1a0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Configuration pour utiliser le serveur llama.cpp local\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"sk-no-key-required\"  # Clé factice, llama.cpp n'en a pas besoin\n",
    ")\n",
    "\n",
    "# Fonction pour vérifier que le serveur est en cours d'exécution\n",
    "def check_server():\n",
    "    try:\n",
    "        base_url_str = str(client.base_url)\n",
    "        \n",
    "        # Supprimer \"/v1\" de l'URL de base pour accéder à \"/models\"\n",
    "        if base_url_str.endswith(\"/v1/\"):\n",
    "            base_url_str = base_url_str[:-3]\n",
    "            \n",
    "        response = requests.get(f\"{base_url_str}/models\")\n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Connexion au serveur llama.cpp réussie!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Le serveur a répondu avec le code {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"❌ Impossible de se connecter au serveur llama.cpp.\")\n",
    "        print(\"Assurez-vous que le serveur est en cours d'exécution sur http://localhost:8080\")\n",
    "        return False\n",
    "\n",
    "# Vérification de la connexion au serveur\n",
    "check_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94070a3-99b8-4548-9659-17e6be04b78f",
   "metadata": {},
   "source": [
    "## 3. Utilisation basique d'un SLM\n",
    "\n",
    "Maintenant que nous sommes connectés au serveur, nous pouvons commencer à interagir avec notre modèle de langage. Commençons par créer une fonction simple pour envoyer des requêtes au modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed481e9d-3f10-4f16-bcfd-84f78e81a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(prompt, temperature=0.7, max_tokens=256):\n",
    "    \"\"\"\n",
    "    Envoie une requête au modèle de langage et retourne sa réponse.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Le texte d'entrée pour le modèle\n",
    "        temperature (float): Contrôle la créativité (0.0 = déterministe, 1.0 = créatif)\n",
    "        max_tokens (int): Nombre maximum de tokens à générer\n",
    "        \n",
    "    Returns:\n",
    "        str: La réponse générée par le modèle\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.completions.create(\n",
    "            model=\"local-model\",  # Le nom du modèle n'est pas important pour llama.cpp\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Erreur lors de la requête au modèle: {str(e)}\"\n",
    "\n",
    "# Test simple\n",
    "prompt = \"Explique ce qu'est l'intelligence artificielle en 3 phrases.\"\n",
    "response = query_llm(prompt)\n",
    "print(f\"Prompt: {prompt}\\n\\nRéponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d65cbf7-2333-4fc5-bbe7-dcd26fd5220c",
   "metadata": {},
   "source": [
    "## 4. Exploration des paramètres\n",
    "\n",
    "Les modèles de langage sont influencés par plusieurs paramètres qui contrôlent la génération de texte. Explorons les deux principaux:\n",
    "\n",
    "### Temperature\n",
    "- Contrôle la \"créativité\" ou l'aléatoire des réponses\n",
    "- Valeurs basses (0.0-0.3): Réponses plus déterministes, cohérentes\n",
    "- Valeurs moyennes (0.4-0.7): Bon équilibre\n",
    "- Valeurs élevées (0.8-1.0): Réponses plus créatives, variées, parfois incohérentes\n",
    "\n",
    "### Max Tokens\n",
    "- Limite la longueur de la réponse générée\n",
    "- Un token représente environ 4 caractères en anglais\n",
    "- Une page de texte = environ 500-700 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405fea3-bcef-44b7-8356-a6f667c2ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testons différentes températures pour la même question\n",
    "prompt = \"Donne-moi une idée de startup innovante\"\n",
    "\n",
    "print(\"Temperature = 0.1 (Conservateur):\")\n",
    "response_conservative = query_llm(prompt, temperature=0.1)\n",
    "print(response_conservative)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Temperature = 0.7 (Équilibré):\")\n",
    "response_balanced = query_llm(prompt, temperature=0.7)\n",
    "print(response_balanced)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Temperature = 1.0 (Créatif):\")\n",
    "response_creative = query_llm(prompt, temperature=1.0)\n",
    "print(response_creative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edd3bc-c742-4f47-9451-57a311eaff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testons différentes limites de tokens\n",
    "prompt = \"Explique en détail comment fonctionne un modèle de langage comme GPT\"\n",
    "\n",
    "print(\"Max Tokens = 50 (Court):\")\n",
    "response_short = query_llm(prompt, max_tokens=50)\n",
    "print(response_short)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Max Tokens = 200 (Moyen):\")\n",
    "response_medium = query_llm(prompt, max_tokens=200)\n",
    "print(response_medium)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Max Tokens = 500 (Long):\")\n",
    "response_long = query_llm(prompt, max_tokens=500)\n",
    "print(response_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a675a-cfa0-44b7-ac62-135084e0da4f",
   "metadata": {},
   "source": [
    "## 5. Exercice pratique\n",
    "\n",
    "Maintenant, c'est à votre tour! Complétez l'exercice suivant pour pratiquer l'utilisation du SLM:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2041c94-a6b8-4948-9ce6-cf7d44d17051",
   "metadata": {},
   "source": [
    "### Exercice : Créer une fonction d'assistant spécialisé\n",
    "Complétez la fonction ci-dessous pour créer un assistant spécialisé dans un domaine particulier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d533db2-0d5c-41f8-a004-7b2e533eebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_specialized_assistant(domain):\n",
    "    \"\"\"\n",
    "    Crée un assistant spécialisé dans un domaine particulier.\n",
    "    \n",
    "    Args:\n",
    "        domain (str): Le domaine de spécialisation (ex: \"programmation\", \"marketing\", \"science\")\n",
    "        \n",
    "    Returns:\n",
    "        function: Une fonction qui prend une question et retourne une réponse de l'assistant\n",
    "    \"\"\"\n",
    "    # À COMPLÉTER: Définir un prompt système approprié selon le domaine\n",
    "    system_prompts = {\n",
    "        \"programmation\": \"Tu es un expert en programmation informatique.\",\n",
    "        # Ajoutez d'autres domaines ici\n",
    "    }\n",
    "    \n",
    "    # À COMPLÉTER: Récupérer le prompt système approprié ou utiliser un par défaut\n",
    "    \n",
    "    def ask(question):\n",
    "        # À COMPLÉTER: Construire le prompt complet et interroger le modèle\n",
    "        pass\n",
    "    \n",
    "    return ask\n",
    "\n",
    "# Test de votre implémentation\n",
    "# Décommentez ces lignes une fois votre fonction complétée\n",
    "# programming_assistant = create_specialized_assistant(\"programmation\")\n",
    "# response = programming_assistant(\"Explique ce qu'est une fonction récursive et donne un exemple simple.\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b27c6-51a5-4183-8de1-f3788e902907",
   "metadata": {},
   "source": [
    "## 7. Conclusion et ressources supplémentaires\n",
    "\n",
    "Dans ce notebook, nous avons:\n",
    "- Compris les différences entre LLMs et SLMs\n",
    "- Appris à interagir avec un SLM local via l'API OpenAI\n",
    "- Exploré les paramètres qui influencent les réponses\n",
    "- Créé des fonctions spécialisées utilisant le SLM\n",
    "\n",
    "### Ressources supplémentaires:\n",
    "- [Guide de prompting OpenAI](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Documentation llama.cpp](https://github.com/ggerganov/llama.cpp)\n",
    "- [Hugging Face - Modèles SLM populaires](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)\n",
    "- [Learn Prompting](https://learnprompting.org/) - Guide complet sur l'art du prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0598b98-a258-46c4-bcff-7fe5f5628437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
